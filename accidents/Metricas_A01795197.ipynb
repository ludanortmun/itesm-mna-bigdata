{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Actividad 4 | Métricas de calidad de resultados",
   "id": "36d0619c56400d92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, hour, date_format, count, round, concat_ws, rand\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, percentile_approx\n"
   ],
   "id": "f6e6e2c4fed5b10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Construcción de la muestra M",
   "id": "aac9bd24bcb37b0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Primero descargaremos el dataset de forma local y creamos la sesión de PySpark.",
   "id": "c5b24782f1b8b2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the latest version\n",
    "path = kagglehub.dataset_download(\"sobhanmoosavi/us-accidents\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "dataset_path = path + \"/US_Accidents_March23.csv\""
   ],
   "id": "c8aac55c4e7bd32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ],
   "id": "f4c982c0e1eb9594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora podemos cargar nuestro dataset como un dataframe de PySpark.",
   "id": "6343e645d92e75e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = spark.read.csv(dataset_path, header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)"
   ],
   "id": "8f106b8ed9e74294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A continuación aplicamos nuestro muestreo del conjunto de datos, tal como se describió en la actividad anterior.\n",
    "\n",
    "El primer paso consiste en crear las columnas Weather_Condition, Hora_Periodo y Tipo_Día."
   ],
   "id": "b8e54f81d0ebcb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df.withColumn(\"Weather_Type\",\n",
    "    when(col(\"Weather_Condition\").isNull(), \"Desconocido\")\n",
    "    .when(col(\"Weather_Condition\").rlike(\"(?i)null|N/A\"), \"Desconocido\")\n",
    "    .when(col(\"Weather_Condition\").rlike(\"(?i)Rain|Drizzle|Thunder|Storm|Snow|Sleet|Hail|Ice|Fog|Haze|Mist|Dust|Sand|Smoke|Wintry|Squall|Tornado|Ash|Funnel\"), \"Adverso\")\n",
    "    .otherwise(\"Favorable\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Hora_Periodo\",\n",
    "    when(hour(\"Start_Time\") < 6, \"Madrugada\")\n",
    "    .when(hour(\"Start_Time\") < 18, \"Alta actividad\")\n",
    "    .otherwise(\"Tarde-Noche\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"Dia_Semana\", date_format(\"Start_Time\", \"E\"))\n",
    "df = df.withColumn(\n",
    "    \"Tipo_Día\",\n",
    "    when(col(\"Dia_Semana\").isin(\"Sat\", \"Sun\"), \"Fin de semana\").otherwise(\"Laboral\")\n",
    ")"
   ],
   "id": "657291697acc8107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Posteriormente, filtramos cualquier registro en el cual las columnas clave (Severity, Hora_Periodo, Tipo_Día y Weather_Type) son nulas.",
   "id": "35554b5d0327426e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_filtrado = df.filter(\n",
    "    (col(\"Severity\").isNotNull()) &\n",
    "    (col(\"Hora_Periodo\").isNotNull()) &\n",
    "    (col(\"Tipo_Día\").isNotNull()) &\n",
    "    (col(\"Weather_Type\").isNotNull())\n",
    ")"
   ],
   "id": "475928c6c383233f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora obtenemos los estratos a partir de las diferentes combinaciones de estas variables, al igual que la probabilidad para cada estrato.",
   "id": "a568fa9483d42fc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_registros = df_filtrado.count()\n",
    "\n",
    "estratos = df_filtrado.groupBy(\"Severity\", \"Hora_Periodo\", \"Tipo_Día\", \"Weather_Type\") \\\n",
    "    .agg(count(\"*\").alias(\"frecuencia\")) \\\n",
    "    .withColumn(\"probabilidad\", round(col(\"frecuencia\") / total_registros, 6)) \\\n",
    "    .orderBy(col(\"probabilidad\").desc())"
   ],
   "id": "7c12029359a779cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para cada estrato, calculamos el número de elementos a incluir a partir del tamaño de la muestra deseado y la probabilidad para cada estrato. En este caso, buscamos una sub-muestra de 10,000 elementos.",
   "id": "5e545efffa9ea52c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tamaño total de muestra deseado\n",
    "n_muestra = 10000\n",
    "\n",
    "estratos = estratos.withColumn(\n",
    "    \"n_estrato\",\n",
    "    round(col(\"probabilidad\") * n_muestra).cast(\"integer\")\n",
    ")"
   ],
   "id": "b24cc9efd416e50a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unimos los dataframes con la información de los estratos con nuestro dataset.",
   "id": "1e1b64b9ab3f39d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# En df_filtrado (base depurada sin nulos en variables clave)\n",
    "df_filtrado = df_filtrado.withColumn(\n",
    "    \"estrato_id\",\n",
    "    concat_ws(\"_\", \"Severity\", \"Hora_Periodo\", \"Tipo_Día\", \"Weather_Type\")\n",
    ")\n",
    "\n",
    "# Igual en la tabla de estratos con probabilidades y n_estrato\n",
    "estratos = estratos.withColumn(\n",
    "    \"estrato_id\",\n",
    "    concat_ws(\"_\", \"Severity\", \"Hora_Periodo\", \"Tipo_Día\", \"Weather_Type\")\n",
    ")\n",
    "\n",
    "df_muestreo = df_filtrado.join(\n",
    "    estratos.select(\"estrato_id\", \"n_estrato\"),\n",
    "    on=\"estrato_id\",\n",
    "    how=\"inner\"\n",
    ")"
   ],
   "id": "aa58838f80c51a3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ordenamos de forma aleatoria los elementos dentro de cada estrato.",
   "id": "6d4231122e864a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Asignar un número aleatorio y calcular el orden por estrato\n",
    "df_muestreo = df_muestreo.withColumn(\"rand\", rand(seed=42))\n",
    "\n",
    "window = Window.partitionBy(\"estrato_id\").orderBy(\"rand\")\n",
    "\n",
    "df_muestreo = df_muestreo.withColumn(\"row_num\", row_number().over(window))"
   ],
   "id": "7ad42b534dd1578f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finalmente, creamos nuestro data frame con la muestra a utilizar, incluyendo sólamente el número de elementos correspondiente a cada estrato.",
   "id": "855c79fda378ce16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_muestra_final = df_muestreo.filter(col(\"row_num\") <= col(\"n_estrato\"))",
   "id": "17fc812df608fde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez construida la muestra, podemos persistir el dataframe para que PySpark optimice las transformaciones posteriores. Al persistir el DataFrame, nos aseguramos de \"congelar\" su estado actual, de modo que cualquier operación posterior tenga un punto de partida definido.",
   "id": "37faf71b4249ef5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_muestra_final = df_muestra_final.persist()",
   "id": "8d2c3fe3d1597e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_muestra_final.summary()",
   "id": "85e7ddfb3e995258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Construcción Train - Test",
   "id": "9ace85ec07ed7c76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Previo a la construcción de los conjuntos train y test, vamos a realizar una limpieza de datos.\n",
    "Esto consiste en:\n",
    "- Eliminar columnas con metadatos que no proporcionarán valor a ningún modelo\n",
    "- Eliminar columnas que son redundantes con otras columnas en el conjunto de datos\n",
    "- Eliminar columnas donde más del 5% de los registros son valores faltantes\n",
    "- De las columnas restantes, eliminar los registros donde existan valores faltantes"
   ],
   "id": "11d04e454f719fa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comenzamos con la limpieza de las columnas agregadas para el sub muestreo\n",
    "cols_to_drop = ['ID', 'estrato_id', 'n_estrato', 'rand', 'row_num', 'rand', 'Source']\n"
   ],
   "id": "ab68c559a24015f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ahora las columnas irrelevantes o redundantes.\n",
    "cols_to_drop += ['Start_Lng', 'End_Lng', 'Start_Lat', 'End_Lat', 'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Wind_Chill(F)', 'Description', 'Wind_Direction', 'Sunrise_Sunset']\n"
   ],
   "id": "8fd48956e45998a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ignoramos las columnas que ya identificamos como columnas a remover\n",
    "cols = [c for c in df_muestra_final.columns if c not in cols_to_drop]\n",
    "total = df_muestra_final.count()\n",
    "\n",
    "for c in cols:\n",
    "    n_missing = df_muestra_final.filter(col(c).isNull()).count()\n",
    "    if n_missing > (total*0.05):\n",
    "        print(f'Dropping column {c}')\n",
    "        cols_to_drop.append(c)\n"
   ],
   "id": "a74d47d15b89a9c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = [c for c in df_muestra_final.columns if c not in cols_to_drop]\n",
    "df_muestra_final = df_muestra_final.dropna(subset=cols)"
   ],
   "id": "e8944d5086cafe2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Una vez realizada la limpieza de datos, podemos proceder a crear nuestros conjuntos de train y test. Para este ejercicio, se utilizará una proporción de 80/20, la cual es comúnmnente utilizada en problemas de aprendizaje de máquina.",
   "id": "5cd169933d2af7de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data,test_data = df_muestra_final.randomSplit([0.8,0.2], seed = 42)\n",
    "print(f\"\"\"Existen {train_data.count()} instancias en el conjunto train, y {test_data.count()} en el conjunto test\"\"\")"
   ],
   "id": "49feea82b4341b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Selección de métricas para medir calidad de resultados",
   "id": "7daa542c847581cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El modelo de aprendizaje automático a crear durante este ejercicio será un modelo de clasificación binaria. La intención de este modelo es predecir si un accidente será de baja o alta severidad, basado en las condiciones bajo las cuales sucedió.\n",
    "\n",
    "El caso de uso hipotético para este modelo es por parte de los equipos de respuesta a accidentes; la intención será que, una vez recibido un reporte de accidente, puedan utilizar este modelo para predecir su severidad y priorizar los recursos de respuesta de forma apropiada e informada.\n",
    "\n",
    "Debido a que se trata de un modelo de clasificación binaria, se utilizarán las siguientes métricas para evaluar la calidad del modelo:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "Además, considerando el contexto del problema, la métrica para la cual se busca optimizar será recall. Esto es debido a que se busca reducir la cantidad de falsos negativos; es decir, reducir la cantidad de accidentes de alta severidad que son catalogados como baja severidad. El razonamiento detrás de esta decisión es que el costo de un falso negativo (no atender inmediatamente un accidente grave) es mayor que el costo de un falso positivo (atender inmediatamente un accidente leve)."
   ],
   "id": "1c1a7ee295f612f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Entrenamiento de Modelos de Aprendizaje",
   "id": "6dd9b5bf6ad2b873"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Antes de comenzar a entrenar nuestro modelo, se realizará preprocesamiento básico del dataset con la finalidad de prepararlo para el entrenamiento.\n",
    "\n",
    "La primera transformación consiste en crear una columna \"Minutes\" calculada a partir del tiempo de inicio y final de los accidentes. En el caso de uso hipotético en el que los equipos de respuesta utilizarán este modelo, la columna se calculará a partir de la hora a la que se reportó el accidente y la hora actual."
   ],
   "id": "11d4c2c82857e107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = train_data.withColumn('Minutes', (col('End_Time').cast('long') - col('Start_Time').cast('long')) / 60)\n",
    "test_data = train_data.withColumn('Minutes', (col('End_Time').cast('long') - col('Start_Time').cast('long')) / 60)"
   ],
   "id": "5632afad29d89851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora procedemos a eliminar outliers mediante la técnica IQR. Esto lo aplicaremos a las columnas Minutes y Distance.",
   "id": "154d6160f39daf0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate Q1 and Q3\n",
    "quantiles = train_data.select(\n",
    "    percentile_approx('Minutes', [0.25, 0.75], 10000).alias('quantiles')\n",
    ").collect()[0]['quantiles']\n",
    "\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers\n",
    "train_data = train_data.filter(\n",
    "    (col('Minutes') >= Q1 - 1.5 * IQR) &\n",
    "    (col('Minutes') <= Q3 + 1.5 * IQR)\n",
    ")\n",
    "test_data = test_data.filter(\n",
    "    (col('Minutes') >= Q1 - 1.5 * IQR) &\n",
    "    (col('Minutes') <= Q3 + 1.5 * IQR)\n",
    ")"
   ],
   "id": "82d143c5eaba7350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate Q1 and Q3\n",
    "quantiles = train_data.select(\n",
    "    percentile_approx('Distance(mi)', [0.25, 0.75], 10000).alias('quantiles')\n",
    ").collect()[0]['quantiles']\n",
    "\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers\n",
    "train_data = train_data.filter(\n",
    "    (col('Distance(mi)') >= Q1 - 1.5 * IQR) &\n",
    "    (col('Distance(mi)') <= Q3 + 1.5 * IQR)\n",
    ")\n",
    "test_data = test_data.filter(\n",
    "    (col('Distance(mi)') >= Q1 - 1.5 * IQR) &\n",
    "    (col('Distance(mi)') <= Q3 + 1.5 * IQR)\n",
    ")"
   ],
   "id": "884e0b4bd86f7433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora procedemos a crear nuestra columna objetivo a partir de la columna Severity. Para este ejercicio, severidades 1 y 2 se considerarán como accidentes leves, mientras que las severidades 3 y 4 se considerarán accidentes graves.",
   "id": "730a8728a4e2d60d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = train_data.withColumn('IsSevere', (col('Severity') > 2).cast(\"bool\"))\n",
    "test_data = test_data.withColumn('IsSevere', (col('Severity') > 2).cast(\"bool\"))"
   ],
   "id": "2ee563d6aeedf781",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora bien, procedemos a crear nuestro vector de características. Para este ejercicio, consideraremos las variables numéricas Distance, Visibility y Minutes; así como las variables categóricas Weather_Type, Hora_Periodo y Tipo_Día.",
   "id": "b4fe92c108c2cfc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para las variables numéricas, aplicamos escalamiento estándar.",
   "id": "162f97037b01a292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols_to_scale = ['Distance(mi)', 'Visibility(mi)', 'Minutes']\n",
    "\n",
    "vectorizer = VectorAssembler(inputCols=cols_to_scale, outputCol=\"numerical_features\")\n",
    "train_data = vectorizer.transform(train_data)\n",
    "test_data = vectorizer.transform(test_data)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\")\n",
    "fitted_scaler = scaler.fit(train_data)\n",
    "train_data = fitted_scaler.transform(train_data)\n",
    "test_data = fitted_scaler.transform(test_data)\n"
   ],
   "id": "bb7cbde2ae8785f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Por su parte, las variables categóricas deben ser indexadas primero, para posteriormente codificar mediante One Hot encoding.",
   "id": "fe721abe4ce2ed3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_columns = ['Weather_Type', 'Dia_Semana', 'Tipo_Día', 'Hora_Periodo']\n",
    "\n",
    "# Primero convertimos todas las columnas a índices\n",
    "for c in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid='keep')\n",
    "    fitted_indexer = indexer.fit(train_data)\n",
    "    train_data = fitted_indexer.transform(train_data)\n",
    "    test_data = fitted_indexer.transform(test_data)"
   ],
   "id": "e9db7eaf61ec30fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_index_cols = [f\"{c}_index\" for c in categorical_columns]\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=categorical_index_cols, outputCols=[f\"{c}_vector\" for c in categorical_columns], handleInvalid='keep')\n",
    "fitted_encoder = encoder.fit(train_data)\n",
    "train_data = fitted_encoder.transform(train_data)\n",
    "test_data = fitted_encoder.transform(test_data)"
   ],
   "id": "64210510ed5aa8e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ahora creamos un solo vector que contenga las características para entrenar nuestro modelo.",
   "id": "3c0ddf708ca545d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols_to_vectorize = ['Weather_Type_vector', 'Tipo_Día_vector', 'Hora_Periodo_vector', 'Severity', 'scaled_features']\n",
    "vectorizer = VectorAssembler(inputCols=cols_to_vectorize, outputCol=\"input_features\")\n",
    "train_data = vectorizer.transform(train_data)\n",
    "test_data = vectorizer.transform(test_data)"
   ],
   "id": "a847686e4f5167bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finalmente, podemos crear un modelo de clasificación.",
   "id": "bd5cfb3377184bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "log_regression = LogisticRegression(featuresCol='input_features', labelCol='IsSevere').fit(train_data)",
   "id": "5d4acd1aaab04419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = log_regression.evaluate(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='IsSevere')\n",
    "result = evaluator.evaluate(predictions.predictions)"
   ],
   "id": "d32b884e7007922d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Análisis de resultados",
   "id": "16ef2c1633dae5c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
